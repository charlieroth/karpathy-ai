{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Makemore Part 3: MLP with Optimizations\n",
    "\n",
    "#### Optimize Initial Loss\n",
    "\n",
    "https://www.youtube.com/watch?v=P6sfmUTpUmc&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=4&t=259s\n",
    "\n",
    "A naive way to initialize a network is to randomly assign the probability of one character\n",
    "following another. This can lead to a high initial loss and then once the network starts to\n",
    "train there will be a dramatic drop.\n",
    "\n",
    "A better way to do this intiialization would be to assign the probabilty of one character\n",
    "following another would be to assign them all a uniform likelihood. For instance, for our 27\n",
    "characters, each character will start off with a 1/27 probability.\n",
    "\n",
    "The effect this has on the neural network is that the network spends more time optimizing the\n",
    "network rather than squashing the weights down to a point at which loss optimization can begin\n",
    "\n",
    "#### Fixing Saturated `tanh`\n",
    "\n",
    "https://www.youtube.com/watch?v=P6sfmUTpUmc&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=4&t=779s\n",
    "\n",
    "To understand that we have a saturated `tanh`, meaning the hidden layer is saturated, we can\n",
    "plot the histogram of hidden layer activation values that represent the hidden layer.\n",
    "\n",
    "```python\n",
    "h_pre_activations = embeddings @ W1 + b1 # hidden layer\n",
    "h = torch.tanh(h_pre_activations) # activation of hidden layer\n",
    "```\n",
    "\n",
    "![Histogram of Hidden Layer Values](images/saturated_tanh_hidden_layer_histogram.png)\n",
    "\n",
    "To the beginner this histogram doesn't carry much meaning. To an experience engineer or researcher\n",
    "this histogram presents a problem for the training of the network. \n",
    "\n",
    "The behavior of `tanh(x)` is a squeezing function that brings value between `-1` and `1`. \n",
    "\n",
    "![tanh function graph](images/tanh_function.png)\n",
    "\n",
    "This might seem fine but if we look at the implementation of the backward pass of `tanh`, we can \n",
    "see that the derivative of `tanh(x)` is `1 - tanh(x)^2`. Therefore, the derivative of `tanh(x)` is\n",
    "very small when `tanh(x)` is close to `1` or `-1`. So when the gradient update happens,\n",
    "`self.grad += (1 - t**2) * out.grad`, the gradient effectively vanishes when values are close to\n",
    "`1` or `-1`. The closer the derivative value of `tanh(x)` is to `0`, the gradient will effectively\n",
    "pass through to the next layer of the network. So the change in the gradient is proportional to\n",
    "change that occurs at `(1 - t**2)`.\n",
    "\n",
    "To visualize how many nuerons are saturated, we can look at the graph that depicts how many neurons\n",
    "have a value that will make the gradient vanish, values close to `-1` or `1`, and how many values\n",
    "will make some proportion of the gradient continue to propagate. In the image, the amount of white\n",
    "shows what proportion of values will make the gradient vanish.\n",
    "\n",
    "![Hidden Layer Activation Values Boolean Tensor Graph](images/hidden_layer_activation_values_boolean_tensor.png)\n",
    "\n",
    "Something to look for in graphs like this, is a single column that is all white. This is called a\n",
    "\"dead neuron\" meaning that the gradient will never propagate through the neuron and the neuron\n",
    "will never enter the activation area of the `tanh` function.\n",
    "\n",
    "This behavior of `tanh` is not unique amongst activation functions. Other activation functions\n",
    "such as `sigmoid`, `leaky ReLU`, `Maxout`, `ReLU` and `ELU` can have the same dangers due to\n",
    "their function behavior. This can be seen by looking at their graphs.\n",
    "\n",
    "After introducing some entropy to the initialization of the network, we can see that the network\n",
    "is able to better produce neurons that will activate the `tanh` function.\n",
    "\n",
    "```python\n",
    "# Entropy introduced on the initialization of the network\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.2\n",
    "b1 = torch.randn((n_hidden,),                     generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn((vocab_size,),                   generator=g) * 0\n",
    "```\n",
    "\n",
    "![Hidden Layer Activation Values Boolean Tensor With Entropy](images/hidden_layer_activation_values_boolean_tensor_updated.png)\n",
    "\n",
    "This will positively impact the performance of the network.\n",
    "\n",
    "### Calculating The Initial Scale: \"Kaiming Init\"\n",
    "\n",
    "The paper [Delving Deep into Rectifiers: Surpassing Human-level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)\n",
    "is a good resource for learning about initializing neural networks. The\n",
    "paper studies other activation functions but it is a good analysis\n",
    "of how to reason about initialization.\n",
    "\n",
    "[PyTorch `torch.nn.init.kaiming_normal`](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_)\n",
    "\n",
    "### Batch Normailzation\n",
    "\n",
    "Batch Normalization is a modern innovation that makes the initiailization\n",
    "techniques of neural networks not as important as they were before. Batch \n",
    "Norm enabled the training of deep neural networks reliably and it \"just\"\n",
    "worked.\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "[Rethinking \"Batch\" in BatchNorm](https://arxiv.org/abs/2105.07576)\n",
    "\n",
    "The insight from batch normalization is that we want the hidden states, hidden layer pre-activation values, to be guassian at\n",
    "initialization. This will ensure that the network doesn't experience saturated hidden layers. So to do this, you just normalize\n",
    "the hidden states to be guassian. This may seem like a crazy thing however because batch normalization is a differentiable operation\n",
    "this means that we can backpropagate through it, returning the idea to sanity.\n",
    "\n",
    "From the paper, *Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*, algorithm 1 \n",
    "(batch normalization) shows how the hidden layer pre-activations can normalized and then scaled & shifted. The scaling and shifting\n",
    "is necessary to allow backpropagation to stear the normalization so that the network can still learn.\n",
    "\n",
    "Batch normalization also introduces a regularizing effect on the logits of the network due to residual entropy introduced by the \n",
    "normalization of previously seen batches of inputs.\n",
    "\n",
    "For it's time, batch normalization worked really well. Researchers are trying to remove this batch normalization and insert other\n",
    "normalization techniques such as *layer normalization*, *group normalization*, *weight normalization*, *spectral normalization* and\n",
    "*self-normalizing neural networks*.\n",
    "\n",
    "To summarize, batch normalization is used to control the statistics of activations in the neural network. Batch norm layers can be\n",
    "scattered throughout a neural network but are typically placed after layers of multiplication (e.g., linear layer or convolutional\n",
    "layer). Batch norm has two parameters, gain and bias, which are trained during back propagation, additionally batch norm has two\n",
    "buffers, mean and standard deviation, which are not trained but are computed during training in a running update fashion (there\n",
    "are techniques for this but the code below uses a simple one for simplicity). The running mean and standard deviation are used\n",
    "later at inference so they don't have to be re-calculated after training.\n",
    "\n",
    "#### Batch Normalization Bias\n",
    "\n",
    "Because batch normalization has it's own bias, the use of a bias on the layer you are normalizing is subtracted out therefore it can be removed. In the case of the code below, `b1` is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([182625, 3]) torch.Size([182625])\n",
      "validation torch.Size([22655, 3]) torch.Size([22655])\n",
      "test torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "def build_dataset(words, label=''):\n",
    "  X, Y = [], [] # inputs, labels\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append (rolling window of context)\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(label, X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_train, Y_train = build_dataset(words[:n1], label='train')\n",
    "X_val, Y_val = build_dataset(words[n1:n2], label='validation')\n",
    "X_test, Y_test = build_dataset(words[n2:], label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3) / ((n_embd * block_size) ** 0.5))\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn((vocab_size,),                   generator=g) * 0\n",
    "\n",
    "# BatchNorm parameters\n",
    "batch_norm_gain = torch.ones((1, n_hidden))\n",
    "batch_norm_bias = torch.zeros((1, n_hidden))\n",
    "batch_norm_mean_running = torch.zeros((1, n_hidden))\n",
    "batch_norm_std_running = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, batch_norm_gain, batch_norm_bias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3239\n",
      "  10000/ 200000: 2.0322\n",
      "  20000/ 200000: 2.5675\n",
      "  30000/ 200000: 2.0125\n",
      "  40000/ 200000: 2.2446\n",
      "  50000/ 200000: 1.8897\n",
      "  60000/ 200000: 2.0785\n",
      "  70000/ 200000: 2.3681\n",
      "  80000/ 200000: 2.2918\n",
      "  90000/ 200000: 2.0238\n",
      " 100000/ 200000: 2.3673\n",
      " 110000/ 200000: 2.3132\n",
      " 120000/ 200000: 1.6414\n",
      " 130000/ 200000: 1.9311\n",
      " 140000/ 200000: 2.2231\n",
      " 150000/ 200000: 2.0027\n",
      " 160000/ 200000: 2.0997\n",
      " 170000/ 200000: 2.4949\n",
      " 180000/ 200000: 2.0199\n",
      " 190000/ 200000: 2.1707\n"
     ]
    }
   ],
   "source": [
    "# optimzation\n",
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "losses = []\n",
    "steps = []\n",
    "for i in range(max_steps):\n",
    "    # ---- Mini-batch Construction ----\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = X_train[ix], Y_train[ix] # batch X, Y\n",
    "    \n",
    "    # ---- Forward Pass ----\n",
    "    emb = C[Xb] # embeded the characters into vectors\n",
    "    emb_cat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # ++++ Linear Layer ++++\n",
    "    h_pre_activations = emb_cat @ W1 # hidden layer pre-activation\n",
    "    # ++++ Batch Norm Layer ++++\n",
    "    batch_norm_mean_i = h_pre_activations.mean(0, keepdim=True)\n",
    "    batch_norm_std_i = h_pre_activations.std(0, keepdim=True)\n",
    "    h_pre_activations = batch_norm_gain * (h_pre_activations - batch_norm_mean_i) / batch_norm_std_i + batch_norm_bias\n",
    "    with torch.no_grad():\n",
    "        # Use torch.no_grad() here to put this outside of the gradient based optimization\n",
    "        #\n",
    "        # Update the running batch norm values to be mostly what they used to be last batch\n",
    "        # with a slight nudge in the direction of the current batch\n",
    "        batch_norm_mean_running = 0.999 * batch_norm_mean_running + 0.001 * batch_norm_mean_i\n",
    "        batch_norm_std_running = 0.999 * batch_norm_std_running + 0.001 * batch_norm_std_i\n",
    "    # ++++ Non-Linearity ++++\n",
    "    h = torch.tanh(h_pre_activations) # hidden layer activation\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # ---- Backward Pass ----\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # ---- Parameter Update ----\n",
    "    lr = 0.1 if i < 100_000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    # ---- Track Statistics ----\n",
    "    if i % 10_000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item())\n",
    "    steps.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0674145221710205\n",
      "val 2.1056838035583496\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    # setup\n",
    "    x, y = {\n",
    "        'train': (X_train, Y_train),\n",
    "        'val': (X_val, Y_val),\n",
    "        'test': (X_test, Y_test)\n",
    "    }[split]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[x] # (N, block_size, n_embd)\n",
    "    emb_cat = emb.view(emb.shape[0], -1) # concat into (N, block_size, n_embd)\n",
    "    h_pre_activations = emb_cat @ W1 # (N, block_size, hidden_size)\n",
    "    # h_pre_activations = batch_norm_gain * (h_pre_activations - h_pre_activations.mean(0, keepdim=True)) / h_pre_activations.std(0, keepdim=True) + batch_norm_bias\n",
    "    h_pre_activations = batch_norm_gain * (h_pre_activations - batch_norm_mean_running) / batch_norm_std_running + batch_norm_bias\n",
    "    h = torch.tanh(h_pre_activations) # (N, hidden_size)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "\n",
    "    # eval\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cathlifatubhl.\n",
      "shkirrsthlitslestadsh.\n",
      "jazhmirfardsahmiqueliz.\n",
      "qumaritzieiirdg.\n",
      "leggyufbmghziriqustzishlistan.\n",
      "vabiusstell.\n",
      "diarisix.\n",
      "khegzifsabbel.\n",
      "zaiirsgyasluysfaadhlanyivariksyah.\n",
      "maboubzettr.\n",
      "jesiahasdannakphladlummygotberksiaghubniq.\n",
      "qlylianaysh.\n",
      "xeinahqueld.\n",
      "bldn.\n",
      "tdy.\n",
      "madthgb.\n",
      "sphlynanyasislytte.\n",
      "conzabbuljadak.\n",
      "khlesdanikosmuseff.\n",
      "kyaq.\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "for _ in range(20):\n",
    "  out = []\n",
    "  context = [0] * block_size # initialize with all '...'\n",
    "  while True:\n",
    "    emb = C[torch.tensor([context])] # (1, block_size, n_embd)\n",
    "    h = torch.tanh(emb.view(1, -1) @ W1)\n",
    "    logits = h @ W2 + b2\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    # sample from distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "    # shift the context window and track the samples\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    # if we sample the special '.' token, break\n",
    "    if ix == 0:\n",
    "      break\n",
    "  \n",
    "  print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchifying Our Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
